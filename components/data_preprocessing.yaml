# PIPELINE DEFINITION
# Name: data-preprocessing
# Description: Preprocesses the data: handles missing values, scales features, and splits into train/test sets.
# Inputs:
#    input_data_path: str
# Outputs:
#    feature_names: list
#    test_data_path: str
#    train_data_path: str
components:
  comp-data-preprocessing:
    executorLabel: exec-data-preprocessing
    inputDefinitions:
      parameters:
        input_data_path:
          description: Path to the input CSV file
          parameterType: STRING
    outputDefinitions:
      parameters:
        feature_names:
          description: List of feature column names
          parameterType: LIST
        test_data_path:
          description: Path to the processed test data
          parameterType: STRING
        train_data_path:
          description: Path to the processed training data
          parameterType: STRING
deploymentSpec:
  executors:
    exec-data-preprocessing:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - data_preprocessing
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'pandas==2.1.4'\
          \ 'scikit-learn==1.3.2' 'numpy==1.24.3' 'kfp==2.0.1' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef data_preprocessing(input_data_path: str) -> NamedTuple('Outputs',\
          \ [\n    ('train_data_path', str),\n    ('test_data_path', str),\n    ('feature_names',\
          \ list)\n]):\n    \"\"\"\n    Preprocesses the data: handles missing values,\
          \ scales features, and splits into train/test sets.\n\n    Args:\n     \
          \   input_data_path: Path to the input CSV file\n\n    Returns:\n      \
          \  train_data_path: Path to the processed training data\n        test_data_path:\
          \ Path to the processed test data\n        feature_names: List of feature\
          \ column names\n\n    This component performs data cleaning, feature scaling\
          \ using StandardScaler,\n    and splits data into 80% training and 20% test\
          \ sets.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n \
          \   from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing\
          \ import StandardScaler\n    import os\n    from collections import namedtuple\n\
          \n    print(\"[DATA PREPROCESSING] Starting data preprocessing...\")\n \
          \   print(f\"[DATA PREPROCESSING] Loading data from: {input_data_path}\"\
          )\n\n    df = pd.read_csv(input_data_path)\n    print(f\"[DATA PREPROCESSING]\
          \ Initial shape: {df.shape}\")\n\n    # Handle missing values\n    initial_rows\
          \ = len(df)\n    df = df.dropna()\n    dropped_rows = initial_rows - len(df)\n\
          \    print(f\"[DATA PREPROCESSING] Dropped {dropped_rows} rows with missing\
          \ values\")\n    print(f\"[DATA PREPROCESSING] Shape after cleaning: {df.shape}\"\
          )\n\n    # Separate features and target\n    X = df.drop('PRICE', axis=1)\n\
          \    y = df['PRICE']\n\n    feature_names = X.columns.tolist()\n    print(f\"\
          [DATA PREPROCESSING] Number of features: {len(feature_names)}\")\n    print(f\"\
          [DATA PREPROCESSING] Features: {feature_names}\")\n\n    # Split the data\
          \ (80% train, 20% test)\n    X_train, X_test, y_train, y_test = train_test_split(\n\
          \        X, y, test_size=0.2, random_state=42\n    )\n    print(f\"[DATA\
          \ PREPROCESSING] Train set size: {len(X_train)}\")\n    print(f\"[DATA PREPROCESSING]\
          \ Test set size: {len(X_test)}\")\n\n    # Scale the features using StandardScaler\n\
          \    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n\
          \    X_test_scaled = scaler.transform(X_test)\n    print(\"[DATA PREPROCESSING]\
          \ Features scaled using StandardScaler\")\n\n    # Convert back to DataFrame\n\
          \    X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names)\n\
          \    X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_names)\n\
          \n    # Save processed data\n    output_dir = \"/tmp/processed_data\"\n\
          \    os.makedirs(output_dir, exist_ok=True)\n\n    train_path = f\"{output_dir}/train_data.csv\"\
          \n    test_path = f\"{output_dir}/test_data.csv\"\n\n    # Combine features\
          \ and target\n    train_df = X_train_scaled.copy()\n    train_df['PRICE']\
          \ = y_train.values\n    test_df = X_test_scaled.copy()\n    test_df['PRICE']\
          \ = y_test.values\n\n    train_df.to_csv(train_path, index=False)\n    test_df.to_csv(test_path,\
          \ index=False)\n\n    print(f\"[DATA PREPROCESSING] Training data saved\
          \ to: {train_path}\")\n    print(f\"[DATA PREPROCESSING] Test data saved\
          \ to: {test_path}\")\n    print(\"[DATA PREPROCESSING] Preprocessing completed\
          \ successfully\")\n\n    outputs = namedtuple('Outputs', ['train_data_path',\
          \ 'test_data_path', 'feature_names'])\n    return outputs(train_path, test_path,\
          \ feature_names)\n\n"
        image: python:3.9
pipelineInfo:
  name: data-preprocessing
root:
  dag:
    outputs:
      parameters:
        feature_names:
          valueFromParameter:
            outputParameterKey: feature_names
            producerSubtask: data-preprocessing
        test_data_path:
          valueFromParameter:
            outputParameterKey: test_data_path
            producerSubtask: data-preprocessing
        train_data_path:
          valueFromParameter:
            outputParameterKey: train_data_path
            producerSubtask: data-preprocessing
    tasks:
      data-preprocessing:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-data-preprocessing
        inputs:
          parameters:
            input_data_path:
              componentInputParameter: input_data_path
        taskInfo:
          name: data-preprocessing
  inputDefinitions:
    parameters:
      input_data_path:
        description: Path to the input CSV file
        parameterType: STRING
  outputDefinitions:
    parameters:
      feature_names:
        description: List of feature column names
        parameterType: LIST
      test_data_path:
        description: Path to the processed test data
        parameterType: STRING
      train_data_path:
        description: Path to the processed training data
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.0.1
