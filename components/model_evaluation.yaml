# PIPELINE DEFINITION
# Name: model-evaluation
# Description: Evaluates the trained model on test data and computes performance metrics.
# Inputs:
#    model_path: str
#    test_data_path: str
# Outputs:
#    accuracy: float
#    mae: float
#    metrics_path: str
#    r2_score: float
#    rmse: float
components:
  comp-model-evaluation:
    executorLabel: exec-model-evaluation
    inputDefinitions:
      parameters:
        model_path:
          description: Path to the saved model file
          parameterType: STRING
        test_data_path:
          description: Path to the test CSV file
          parameterType: STRING
    outputDefinitions:
      parameters:
        accuracy:
          description: Percentage of predictions within 20% of actual value
          parameterType: NUMBER_DOUBLE
        mae:
          description: Mean Absolute Error
          parameterType: NUMBER_DOUBLE
        metrics_path:
          description: Path to the JSON file containing all metrics
          parameterType: STRING
        r2_score:
          description: R-squared score (coefficient of determination)
          parameterType: NUMBER_DOUBLE
        rmse:
          description: Root Mean Squared Error
          parameterType: NUMBER_DOUBLE
deploymentSpec:
  executors:
    exec-model-evaluation:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - model_evaluation
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'pandas==2.1.4'\
          \ 'scikit-learn==1.3.2' 'joblib==1.3.2' 'kfp==2.0.1' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef model_evaluation(model_path: str, test_data_path: str) -> NamedTuple('Outputs',\
          \ [\n    ('accuracy', float),\n    ('rmse', float),\n    ('r2_score', float),\n\
          \    ('mae', float),\n    ('metrics_path', str)\n]):\n    \"\"\"\n    Evaluates\
          \ the trained model on test data and computes performance metrics.\n\n \
          \   Args:\n        model_path: Path to the saved model file\n        test_data_path:\
          \ Path to the test CSV file\n\n    Returns:\n        accuracy: Percentage\
          \ of predictions within 20% of actual value\n        rmse: Root Mean Squared\
          \ Error\n        r2_score: R-squared score (coefficient of determination)\n\
          \        mae: Mean Absolute Error\n        metrics_path: Path to the JSON\
          \ file containing all metrics\n\n    This component loads the trained model,\
          \ makes predictions on test data,\n    and calculates various regression\
          \ metrics to assess model performance.\n    \"\"\"\n    import pandas as\
          \ pd\n    import joblib\n    from sklearn.metrics import mean_squared_error,\
          \ r2_score, mean_absolute_error\n    import json\n    import os\n    from\
          \ collections import namedtuple\n    import numpy as np\n\n    print(\"\
          [MODEL EVALUATION] Starting model evaluation...\")\n    print(f\"[MODEL\
          \ EVALUATION] Loading model from: {model_path}\")\n    model = joblib.load(model_path)\n\
          \n    print(f\"[MODEL EVALUATION] Loading test data from: {test_data_path}\"\
          )\n    test_df = pd.read_csv(test_data_path)\n    print(f\"[MODEL EVALUATION]\
          \ Test data shape: {test_df.shape}\")\n\n    # Separate features and target\n\
          \    X_test = test_df.drop('PRICE', axis=1)\n    y_test = test_df['PRICE']\n\
          \n    print(\"[MODEL EVALUATION] Making predictions on test data...\")\n\
          \    y_pred = model.predict(X_test)\n\n    # Calculate regression metrics\n\
          \    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    mae = mean_absolute_error(y_test,\
          \ y_pred)\n    r2 = r2_score(y_test, y_pred)\n\n    # Calculate custom \"\
          accuracy\" metric for regression\n    # Percentage of predictions within\
          \ 20% of actual value\n    accuracy = np.mean(np.abs((y_test - y_pred) /\
          \ y_test) < 0.2) * 100\n\n    # Calculate additional statistics\n    mean_price\
          \ = y_test.mean()\n    std_price = y_test.std()\n\n    metrics = {\n   \
          \     'rmse': float(rmse),\n        'mae': float(mae),\n        'r2_score':\
          \ float(r2),\n        'accuracy_within_20_percent': float(accuracy),\n \
          \       'mean_actual_price': float(mean_price),\n        'std_actual_price':\
          \ float(std_price),\n        'num_test_samples': len(y_test)\n    }\n\n\
          \    print(\"\\n\" + \"=\"*50)\n    print(\"[MODEL EVALUATION] EVALUATION\
          \ RESULTS\")\n    print(\"=\"*50)\n    print(f\"  Root Mean Squared Error\
          \ (RMSE): ${rmse:,.2f}\")\n    print(f\"  Mean Absolute Error (MAE):   \
          \   ${mae:,.2f}\")\n    print(f\"  R\xB2 Score:                        {r2:.4f}\"\
          )\n    print(f\"  Accuracy (within 20%):           {accuracy:.2f}%\")\n\
          \    print(f\"  Number of test samples:          {len(y_test)}\")\n    print(f\"\
          \  Mean actual price:               ${mean_price:,.2f}\")\n    print(\"\
          =\"*50 + \"\\n\")\n\n    # Save metrics to JSON file\n    output_dir = \"\
          /tmp/metrics\"\n    os.makedirs(output_dir, exist_ok=True)\n    metrics_path\
          \ = f\"{output_dir}/metrics.json\"\n\n    with open(metrics_path, 'w') as\
          \ f:\n        json.dump(metrics, f, indent=2)\n\n    print(f\"[MODEL EVALUATION]\
          \ Metrics saved to: {metrics_path}\")\n    print(\"[MODEL EVALUATION] Evaluation\
          \ completed successfully\")\n\n    outputs = namedtuple('Outputs', ['accuracy',\
          \ 'rmse', 'r2_score', 'mae', 'metrics_path'])\n    return outputs(accuracy,\
          \ rmse, r2, mae, metrics_path)\n\n"
        image: python:3.9
pipelineInfo:
  name: model-evaluation
root:
  dag:
    outputs:
      parameters:
        accuracy:
          valueFromParameter:
            outputParameterKey: accuracy
            producerSubtask: model-evaluation
        mae:
          valueFromParameter:
            outputParameterKey: mae
            producerSubtask: model-evaluation
        metrics_path:
          valueFromParameter:
            outputParameterKey: metrics_path
            producerSubtask: model-evaluation
        r2_score:
          valueFromParameter:
            outputParameterKey: r2_score
            producerSubtask: model-evaluation
        rmse:
          valueFromParameter:
            outputParameterKey: rmse
            producerSubtask: model-evaluation
    tasks:
      model-evaluation:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-model-evaluation
        inputs:
          parameters:
            model_path:
              componentInputParameter: model_path
            test_data_path:
              componentInputParameter: test_data_path
        taskInfo:
          name: model-evaluation
  inputDefinitions:
    parameters:
      model_path:
        description: Path to the saved model file
        parameterType: STRING
      test_data_path:
        description: Path to the test CSV file
        parameterType: STRING
  outputDefinitions:
    parameters:
      accuracy:
        description: Percentage of predictions within 20% of actual value
        parameterType: NUMBER_DOUBLE
      mae:
        description: Mean Absolute Error
        parameterType: NUMBER_DOUBLE
      metrics_path:
        description: Path to the JSON file containing all metrics
        parameterType: STRING
      r2_score:
        description: R-squared score (coefficient of determination)
        parameterType: NUMBER_DOUBLE
      rmse:
        description: Root Mean Squared Error
        parameterType: NUMBER_DOUBLE
schemaVersion: 2.1.0
sdkVersion: kfp-2.0.1
